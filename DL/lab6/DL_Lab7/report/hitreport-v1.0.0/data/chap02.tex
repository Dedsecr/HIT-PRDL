% !TeX root = ../hitreport-example.tex

\chapter{实验内容}

\section{方案设计}

\subsection{网络结构}
DnCNN模型的网络结构如图~\ref{fig:architecture}所示。该网络是一个使用3$\times$3大小卷积核，并拥有17层卷积层的卷积神经网络。输入层使用64个$3 \times 3 \times c$卷积核将输入图像映射为64通道的特征图，然后使用ReLU作为激活函数。随后是15个重复的卷积层，每层卷积都使用64个$ 3 \times 3 \times 64 $的卷积核进行卷积，使得中间层的通道数始终保持在64，并且在每次卷积之后都加入BN（Batch Normalization）层和激活层。输出层使用1个$ 3 \times 3 \times 64 $的卷积核将64通道的特征图最终映射为一个单通道的输出。

假设带有噪声的图像为$ \boldsymbol{x} $，模型期望通过映射$ \mathcal{H}(\cdot) $将图片映射成去噪后的图像$ \mathcal{H}(\boldsymbol{x}) $。但是由于噪声相对于原本的像素值来说过于微小，这一映射十分接近于恒等映射，这使得模型学习的难度大大增加。因此，该模型选择使用残差学习（Residual Learning）的方法，不直接学习映射$ \mathcal{H}(\cdot) $，转而学习一个残差映射$ \mathcal{F}(\cdot) $，使得$ \mathcal{F}(\boldsymbol{x}) = \mathcal{H}(\boldsymbol{x}) - \boldsymbol{x} $，即学习去噪后的图像与噪声图之间的残差。由于我们规定施加的噪声为加性高斯白噪声（AWGN），因此该残差刚好可以看作是施加的噪声值的相反数。本质上讲，该模型实际上是在学习图像上的噪声$ -\mathcal{F}(\boldsymbol{x}) $，而在进行去噪时，计算$ \mathcal{F}(\boldsymbol{x}) + \boldsymbol{x} = \mathcal{H}(\boldsymbol{x}) $即可得到去噪后的图像$ \mathcal{H}(\boldsymbol{x}) $。

该网络中还使用了批标准化（Batch Normalization）以防止样本内部出现的协方差漂移（Internal Covariate Shift）现象，有益于深度较高的网络的训练。况且，由于残差学习的使用，模型实际上是在学习施加在图像上的高斯噪声，而噪声本身就是符合高斯分布的，因此十分有利于批标准化操作的实行。
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/architecture}
	\caption{网络结构示意图}
	\label{fig:architecture}
\end{figure}

\subsection{损失函数}
模型虽然学习的是残差，即施加在图像上的噪声，但最终输出的是将噪声图减去该残差后得到的去噪后图像。我们选择的损失函数应当能够衡量该输出的去噪后图像与原本的ground-truth图像的差异。因此，可以选用均方误差损失函数（MSE），具体计算公式见式~\ref{function:mse}。其中$ \boldsymbol{x} $表示ground-truth图像的像素值组成的矩阵，$ \boldsymbol{\hat{x}} $表示去噪后图像的像素值组成的矩阵，$ \boldsymbol{x}_{i, j} $表示$ \boldsymbol{x} $的第$ i $行第$ j $列处的像素值。

\begin{align} \label{function:mse}
	MSE = \frac{1}{h \times w}\sum_{i=0}^{h-1}{\sum_{j=0}^{w-1}{\Vert \boldsymbol{x}(i,j)-\boldsymbol{\hat{x}}(i,j)\Vert^2}}
\end{align}


\section{实验过程}

\subsection{DnCNN结构的实现}
本实验首先基于PyTorch代码实现了DnCNN网络结构，具体代码见图~\ref{fig:modelcode}。由于DnCNN网络中有15层重复的卷积层，因此在构造网络时使用了一个循环，不断地向层列表中加入对应的模块，最后再将层列表转换为顺序结构。

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/model_code}
	\caption{}
	\label{fig:modelcode}
\end{figure}

\subsection{训练方案}
对模型训练时使用ADAM优化器，初始学习率设置为0.001，经过30个epochs后学习率减少到0.0002，共训练50个epochs，训练数据的batch size设置为128。训练过程中，由于我们希望每次施加在训练图像上的噪声都是不同的，因此可以将对各个patch加高斯噪声的工作放在Dataset对象的\_\_getitem\_\_方法中，在抽取数据的时候实时添加高斯噪声。具体实现代码如图~\ref{fig:dataset}所示。

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figures/dataset}
	\caption{为patch实时添加噪声的代码实现}
	\label{fig:dataset}
\end{figure}


训练过程中，对每个epoch的loss记录log，并使用tensorboard画出loss随epoch变化的曲线，最终曲线如图~\ref{fig:losscurve}所示。可见，模型在训练到第35个epoch左右时成功收敛，此时的MSE损失约为1.27。

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/loss_curve}
	\caption{使用TensorBoard画出的epoch-loss曲线}
	\label{fig:losscurve}
\end{figure}

\subsection{测试方案}

测试时，首先要从训练时保存的文件中读取模型的参数。接下来，对于每一个测试的数据集，我们读取图片，并按照给训练集数据添加噪声的方案来处理每张图片。再将图片输入到模型中，得到降噪后的图片。最后计算得到的图片和原图片的峰值信噪比（PSNR）和结构相似性（SSIM），并记录测试结果。

测试结果见第~\ref{chapter:res}章。

